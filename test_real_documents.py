#!/usr/bin/env python3
"""
    --- AUTO-GENERATED DOCSTRING ---
    Table of content is automatically generated by Agent Docstrings v1.3.5
    
    Classes/Functions:
        - test_real_document_processing() (line 31)
        - test_mathematical_processing_detailed() (line 188)
    --- END AUTO-GENERATED DOCSTRING ---

Test Enhanced SciRAG with Real Scientific Documents

This script tests the Enhanced SciRAG system with actual scientific papers
from the txt_files directory to validate end-to-end functionality.
"""
from scirag.enhanced_processing.enhanced_chunk import ContentType
from scirag.enhanced_processing import (
    MathematicalProcessor, ContentClassifier, EnhancedChunker,
    AssetProcessor, GlossaryExtractor, EnhancedDocumentProcessor
)
import os
import sys
import time
from pathlib import Path

# Add the project root to the Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


def test_real_document_processing():
    """Test Enhanced SciRAG with real scientific documents."""
    print("üß™ Testing Enhanced SciRAG with Real Scientific Documents")
    print("=" * 60)

    # Initialize processors
    print("\n1. Initializing Enhanced Processing Components...")
    try:
        math_processor = MathematicalProcessor(enable_sympy=True)
        content_classifier = ContentClassifier()
        enhanced_chunker = EnhancedChunker(
            chunk_size=1000,
            overlap_ratio=0.2,
            preserve_math=True,
            preserve_figures=True,
            preserve_tables=True
        )
        asset_processor = AssetProcessor()
        glossary_extractor = GlossaryExtractor()

        print("   ‚úÖ All processors initialized successfully")
    except Exception as e:
        print(f"   ‚ùå Error initializing processors: {e}")
        return False

    # Test with real documents
    txt_files_dir = project_root / "txt_files"
    if not txt_files_dir.exists():
        print(f"   ‚ùå txt_files directory not found: {txt_files_dir}")
        return False

    txt_files = list(txt_files_dir.glob("*.txt"))
    if not txt_files:
        print(f"   ‚ùå No .txt files found in {txt_files_dir}")
        return False

    print(f"   üìÅ Found {len(txt_files)} test documents")

    # Process each document
    total_chunks = 0
    total_math_content = 0
    total_assets = 0
    total_glossary_terms = 0
    processing_times = []

    for i, txt_file in enumerate(txt_files[:3]):  # Test first 3 files
        print(f"\n2.{i+1} Processing: {txt_file.name}")

        try:
            start_time = time.time()

            # Read the document
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()

            print(f"   üìÑ Document size: {len(content):,} characters")

            # Process with enhanced chunker
            chunks = enhanced_chunker.chunk_text(
                text=content,
                source_id=txt_file.stem
            )

            processing_time = time.time() - start_time
            processing_times.append(processing_time)

            print(f"   ‚è±Ô∏è  Processing time: {processing_time:.2f}s")
            print(f"   üì¶ Generated {len(chunks)} enhanced chunks")

            # Analyze chunk content
            math_chunks = [c for c in chunks if c.is_mathematical()]
            asset_chunks = [c for c in chunks if c.is_asset()]
            glossary_chunks = [c for c in chunks if c.is_glossary()]

            print(f"   üßÆ Mathematical chunks: {len(math_chunks)}")
            print(f"   üñºÔ∏è  Asset chunks: {len(asset_chunks)}")
            print(f"   üìö Glossary chunks: {len(glossary_chunks)}")

            # Show sample mathematical content
            if math_chunks:
                sample_math = math_chunks[0]
                print(
                    f"   üî¢ Sample equation: {sample_math.mathematical_content.equation_tex}")
                print(
                    f"   üìä Complexity: {sample_math.mathematical_content.complexity_score:.2f}")

            # Show sample asset content
            if asset_chunks:
                sample_asset = asset_chunks[0]
                print(
                    f"   üñºÔ∏è  Sample asset: {sample_asset.asset_content.asset_type}")
                print(
                    f"   üìù Caption: {sample_asset.asset_content.caption[:100]}...")

            # Show sample glossary content
            if glossary_chunks:
                sample_glossary = glossary_chunks[0]
                print(
                    f"   üìö Sample term: {sample_glossary.glossary_content.term}")
                print(
                    f"   üìñ Definition: {sample_glossary.glossary_content.definition[:100]}...")

            total_chunks += len(chunks)
            total_math_content += len(math_chunks)
            total_assets += len(asset_chunks)
            total_glossary_terms += len(glossary_chunks)

            print(f"   ‚úÖ Successfully processed {txt_file.name}")

        except Exception as e:
            print(f"   ‚ùå Error processing {txt_file.name}: {e}")
            continue

    # Summary
    print(f"\n3. Processing Summary")
    print("=" * 30)
    print(f"üìÑ Documents processed: {min(3, len(txt_files))}")
    print(f"üì¶ Total chunks created: {total_chunks}")
    print(f"üßÆ Mathematical content: {total_math_content}")
    print(f"üñºÔ∏è  Assets identified: {total_assets}")
    print(f"üìö Glossary terms: {total_glossary_terms}")
    print(
        f"‚è±Ô∏è  Average processing time: {sum(processing_times)/len(processing_times):.2f}s")

    # Test content type distribution
    print(f"\n4. Content Type Analysis")
    print("=" * 30)
    content_types = {}
    for txt_file in txt_files[:3]:
        try:
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()

            chunks = enhanced_chunker.chunk_text(
                text=content,
                source_id=txt_file.stem
            )

            for chunk in chunks:
                content_type = chunk.content_type.value
                content_types[content_type] = content_types.get(
                    content_type, 0) + 1

        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error analyzing {txt_file.name}: {e}")
            continue

    for content_type, count in sorted(content_types.items()):
        print(f"   {content_type}: {count}")

    print(f"\nüéâ Real Document Processing Test Completed Successfully!")
    print(f"   Enhanced SciRAG successfully processed scientific documents")
    print(f"   with mathematical content, assets, and glossary extraction.")

    return True


def test_mathematical_processing_detailed():
    """Test detailed mathematical processing on real content."""
    print(f"\n5. Detailed Mathematical Processing Test")
    print("=" * 40)

    math_processor = MathematicalProcessor(enable_sympy=True)

    # Test equations from real documents
    test_equations = [
        "E = mc^2",
        "\\frac{d}{dx}[f(x)] = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}",
        "\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}",
        "\\sum_{i=1}^{n} i = \\frac{n(n+1)}{2}",
        "\\nabla \\times \\vec{E} = -\\frac{\\partial \\vec{B}}{\\partial t}"
    ]

    for i, equation in enumerate(test_equations):
        print(f"\n   Testing equation {i+1}: {equation}")
        try:
            result = math_processor.process_equation(equation)
            print(f"   ‚úÖ Type: {result['equation_type']}")
            print(f"   ‚úÖ Normalized: {result['math_norm']}")
            print(f"   ‚úÖ Complexity: {result['complexity_score']:.2f}")
            print(f"   ‚úÖ Tokens: {len(result['math_tokens'])}")
        except Exception as e:
            print(f"   ‚ùå Error: {e}")


if __name__ == "__main__":
    print("üöÄ Enhanced SciRAG Real Document Processing Test")
    print("=" * 60)

    try:
        # Test real document processing
        success = test_real_document_processing()

        if success:
            # Test detailed mathematical processing
            test_mathematical_processing_detailed()

            print(f"\n‚úÖ All tests passed! Enhanced SciRAG is ready for production use.")
        else:
            print(f"\n‚ùå Some tests failed. Please check the errors above.")
            sys.exit(1)

    except Exception as e:
        print(f"\n‚ùå Test suite failed with error: {e}")
        sys.exit(1)