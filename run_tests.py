#!/usr/bin/env python3
"""
    --- AUTO-GENERATED DOCSTRING ---
    Table of content is automatically generated by Agent Docstrings v1.3.5
    
    Classes/Functions:
        - TestRunner (line 47):
            - run_all_tests(coverage: bool = False) -> Dict[str, Any] (line 55)
            - run_unit_tests() -> Dict[str, Any] (line 126)
            - run_integration_tests() -> Dict[str, Any] (line 212)
            - run_performance_tests() -> Dict[str, Any] (line 253)
            - run_real_document_tests() -> Dict[str, Any] (line 301)
            - run_backward_compatibility_tests() -> Dict[str, Any] (line 349)
            - run_error_handling_tests() -> Dict[str, Any] (line 376)
            - run_configuration_tests() -> Dict[str, Any] (line 408)
            - run_coverage_report() (line 438)
        - main() (line 455)
    --- END AUTO-GENERATED DOCSTRING ---

Enhanced SciRAG Consolidated Testing Suite

This is the main testing entry point for the Enhanced SciRAG package.
It provides a unified testing strategy that covers all aspects of the system.

Usage:
    python run_tests.py                    # Run all tests
    python run_tests.py --unit             # Run only unit tests
    python run_tests.py --integration      # Run only integration tests
    python run_tests.py --performance      # Run only performance tests
    python run_tests.py --quick            # Run quick smoke tests
    python run_tests.py --coverage         # Run with coverage reporting
"""
import sys
import os
import argparse
import time
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional
import json

# Add the project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


class TestRunner:
    """Main test runner for Enhanced SciRAG."""

    def __init__(self):
        self.project_root = project_root
        self.test_results = {}
        self.start_time = time.time()

    def run_all_tests(self, coverage: bool = False) -> Dict[str, Any]:
        """Run all test suites."""
        print("🚀 Enhanced SciRAG Comprehensive Testing Suite")
        print("=" * 60)

        test_suites = [
            ("Unit Tests", self.run_unit_tests),
            ("Integration Tests", self.run_integration_tests),
            ("Performance Tests", self.run_performance_tests),
            ("Real Document Tests", self.run_real_document_tests),
            ("Backward Compatibility Tests", self.run_backward_compatibility_tests),
            ("Error Handling Tests", self.run_error_handling_tests),
            ("Configuration Tests", self.run_configuration_tests),
        ]

        results = {}
        total_passed = 0
        total_failed = 0

        for suite_name, test_func in test_suites:
            print(f"\n📋 Running {suite_name}...")
            print("-" * 40)

            try:
                suite_result = test_func()
                results[suite_name] = suite_result

                if suite_result['passed']:
                    print(
                        f"✅ {suite_name}: PASSED ({suite_result['tests_passed']}/{suite_result['total_tests']})")
                    total_passed += suite_result['tests_passed']
                else:
                    print(
                        f"❌ {suite_name}: FAILED ({suite_result['tests_passed']}/{suite_result['total_tests']})")
                    total_failed += suite_result['tests_failed']

            except Exception as e:
                print(f"💥 {suite_name}: ERROR - {e}")
                results[suite_name] = {
                    'passed': False,
                    'error': str(e),
                    'tests_passed': 0,
                    'tests_failed': 1,
                    'total_tests': 1
                }
                total_failed += 1

        # Generate summary
        total_tests = total_passed + total_failed
        duration = time.time() - self.start_time

        print(f"\n📊 Test Summary")
        print("=" * 30)
        print(f"Total Tests: {total_tests}")
        print(f"Passed: {total_passed}")
        print(f"Failed: {total_failed}")
        print(f"Success Rate: {(total_passed/total_tests*100):.1f}%")
        print(f"Duration: {duration:.2f}s")

        if coverage:
            self.run_coverage_report()

        return {
            'total_tests': total_tests,
            'passed': total_passed,
            'failed': total_failed,
            'success_rate': total_passed / total_tests * 100,
            'duration': duration,
            'suites': results
        }

    def run_unit_tests(self) -> Dict[str, Any]:
        """Run unit tests for individual components."""
        print("🧪 Testing individual components...")

        # Test mathematical processor
        try:
            from scirag.enhanced_processing import MathematicalProcessor
            processor = MathematicalProcessor(enable_sympy=True)
            result = processor.process_equation("E = mc^2")
            assert result['equation_type'] == 'equation'
            assert result['complexity_score'] > 0
            print("  ✅ MathematicalProcessor")
        except Exception as e:
            print(f"  ❌ MathematicalProcessor: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        # Test content classifier
        try:
            from scirag.enhanced_processing import ContentClassifier
            classifier = ContentClassifier()
            content_type = classifier.classify_content(
                "This is prose text.", {})
            assert content_type is not None
            print("  ✅ ContentClassifier")
        except Exception as e:
            print(f"  ❌ ContentClassifier: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        # Test enhanced chunker
        try:
            from scirag.enhanced_processing import EnhancedChunker
            chunker = EnhancedChunker(chunk_size=500, overlap_ratio=0.1)
            chunks = chunker.chunk_text("This is a test document.", "test")
            assert len(chunks) > 0
            print("  ✅ EnhancedChunker")
        except Exception as e:
            print(f"  ❌ EnhancedChunker: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        # Test asset processor
        try:
            from scirag.enhanced_processing import AssetProcessor
            processor = AssetProcessor()
            result = processor.process_asset("Figure 1: Test figure", "test")
            print("  ✅ AssetProcessor")
        except Exception as e:
            print(f"  ❌ AssetProcessor: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        # Test glossary extractor
        try:
            from scirag.enhanced_processing import GlossaryExtractor
            extractor = GlossaryExtractor()
            terms = extractor.extract_glossary_terms(
                "The term 'energy' refers to...", "test")
            print("  ✅ GlossaryExtractor")
        except Exception as e:
            print(f"  ❌ GlossaryExtractor: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        return {
            'passed': True,
            'tests_passed': 5,
            'tests_failed': 0,
            'total_tests': 5}

    def run_integration_tests(self) -> Dict[str, Any]:
        """Run integration tests."""
        print("🔗 Testing component integration...")

        try:
            # Test enhanced SciRAG initialization
            from scirag import SciRagEnhanced
            enhanced_scirag = SciRagEnhanced(
                enable_enhanced_processing=True,
                enable_mathematical_processing=True,
                enable_asset_processing=True,
                enable_glossary_extraction=True
            )
            print("  ✅ SciRagEnhanced initialization")
        except Exception as e:
            print(f"  ❌ SciRagEnhanced initialization: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        try:
            # Test imports
            from scirag import SciRag, SciRagEnhanced
            from scirag.enhanced_processing import MathematicalProcessor
            print("  ✅ All imports successful")
        except Exception as e:
            print(f"  ❌ Import test: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        return {
            'passed': True,
            'tests_passed': 2,
            'tests_failed': 0,
            'total_tests': 2}

    def run_performance_tests(self) -> Dict[str, Any]:
        """Run performance tests."""
        print("⚡ Testing performance...")

        try:
            import time
            from scirag.enhanced_processing import MathematicalProcessor

            # Test processing speed
            processor = MathematicalProcessor(enable_sympy=True)
            start_time = time.time()

            # Process multiple equations
            equations = [
                "E = mc^2",
                "\\frac{d}{dx}[f(x)] = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}",
                "\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}",
                "\\sum_{i=1}^{n} i = \\frac{n(n+1)}{2}",
                "\\nabla \\times \\vec{E} = -\\frac{\\partial \\vec{B}}{\\partial t}"]

            for equation in equations:
                result = processor.process_equation(equation)
                assert result is not None

            processing_time = time.time() - start_time
            print(
                f"  ✅ Processing speed: {processing_time:.3f}s for {len(equations)} equations")

            # Test memory usage
            import psutil
            process = psutil.Process()
            memory_usage = process.memory_info().rss / 1024 / 1024  # MB
            print(f"  ✅ Memory usage: {memory_usage:.1f} MB")

        except Exception as e:
            print(f"  ❌ Performance test: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        return {
            'passed': True,
            'tests_passed': 2,
            'tests_failed': 0,
            'total_tests': 2}

    def run_real_document_tests(self) -> Dict[str, Any]:
        """Run tests with real scientific documents."""
        print("📄 Testing with real documents...")

        try:
            # Check if test documents exist
            txt_files_dir = self.project_root / "txt_files"
            if not txt_files_dir.exists():
                print("  ⚠️  No test documents found, skipping real document tests")
                return {
                    'passed': True,
                    'tests_passed': 0,
                    'tests_failed': 0,
                    'total_tests': 0}

            txt_files = list(txt_files_dir.glob("*.txt"))
            if not txt_files:
                print("  ⚠️  No .txt files found, skipping real document tests")
                return {
                    'passed': True,
                    'tests_passed': 0,
                    'tests_failed': 0,
                    'total_tests': 0}

            # Test with first document
            from scirag.enhanced_processing import EnhancedChunker
            chunker = EnhancedChunker(chunk_size=1000, overlap_ratio=0.2)

            with open(txt_files[0], 'r', encoding='utf-8') as f:
                content = f.read()

            chunks = chunker.chunk_text(content, txt_files[0].stem)
            print(f"  ✅ Processed {txt_files[0].name}: {len(chunks)} chunks")

        except Exception as e:
            print(f"  ❌ Real document test: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        return {
            'passed': True,
            'tests_passed': 1,
            'tests_failed': 0,
            'total_tests': 1}

    def run_backward_compatibility_tests(self) -> Dict[str, Any]:
        """Run backward compatibility tests."""
        print("🔄 Testing backward compatibility...")

        try:
            # Test original SciRAG imports
            from scirag import SciRag
            print("  ✅ Original SciRag import")

            # Test that original functionality still works
            scirag = SciRag()
            print("  ✅ Original SciRag initialization")

        except Exception as e:
            print(f"  ❌ Backward compatibility test: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        return {
            'passed': True,
            'tests_passed': 2,
            'tests_failed': 0,
            'total_tests': 2}

    def run_error_handling_tests(self) -> Dict[str, Any]:
        """Run error handling tests."""
        print("🛡️  Testing error handling...")

        try:
            from scirag.enhanced_processing import MathematicalProcessor

            # Test with invalid input
            processor = MathematicalProcessor(enable_sympy=True)
            result = processor.process_equation("")
            assert result is not None
            print("  ✅ Empty equation handling")

            # Test with malformed input
            result = processor.process_equation("invalid latex \\invalid{")
            assert result is not None
            print("  ✅ Malformed input handling")

        except Exception as e:
            print(f"  ❌ Error handling test: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        return {
            'passed': True,
            'tests_passed': 2,
            'tests_failed': 0,
            'total_tests': 2}

    def run_configuration_tests(self) -> Dict[str, Any]:
        """Run configuration tests."""
        print("⚙️  Testing configuration...")

        try:
            from scirag.config import enhanced_config

            # Test configuration access
            assert hasattr(enhanced_config, 'ENABLE_ENHANCED_PROCESSING')
            assert hasattr(enhanced_config, 'ENABLE_MATHEMATICAL_PROCESSING')
            print("  ✅ Configuration access")

            # Test configuration validation
            errors = enhanced_config.validate_config()
            print(f"  ✅ Configuration validation: {len(errors)} errors")

        except Exception as e:
            print(f"  ❌ Configuration test: {e}")
            return {
                'passed': False,
                'tests_passed': 0,
                'tests_failed': 1,
                'total_tests': 1}

        return {
            'passed': True,
            'tests_passed': 2,
            'tests_failed': 0,
            'total_tests': 2}

    def run_coverage_report(self):
        """Generate coverage report."""
        print("\n📊 Generating coverage report...")
        try:
            # This would require pytest-cov to be installed
            subprocess.run([
                sys.executable, "-m", "pytest",
                "--cov=scirag",
                "--cov-report=html",
                "--cov-report=term"
            ], check=True)
        except subprocess.CalledProcessError:
            print("  ⚠️  Coverage reporting failed (pytest-cov not installed)")
        except FileNotFoundError:
            print("  ⚠️  pytest not found")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Enhanced SciRAG Testing Suite")
    parser.add_argument(
        "--unit",
        action="store_true",
        help="Run only unit tests")
    parser.add_argument(
        "--integration",
        action="store_true",
        help="Run only integration tests")
    parser.add_argument(
        "--performance",
        action="store_true",
        help="Run only performance tests")
    parser.add_argument(
        "--quick",
        action="store_true",
        help="Run quick smoke tests")
    parser.add_argument(
        "--coverage",
        action="store_true",
        help="Run with coverage reporting")

    args = parser.parse_args()

    runner = TestRunner()

    if args.quick:
        # Run only essential tests
        print("🚀 Quick Smoke Tests")
        print("=" * 30)
        result = runner.run_unit_tests()
        if result['passed']:
            print("✅ Quick tests passed!")
            sys.exit(0)
        else:
            print("❌ Quick tests failed!")
            sys.exit(1)
    elif args.unit:
        result = runner.run_unit_tests()
    elif args.integration:
        result = runner.run_integration_tests()
    elif args.performance:
        result = runner.run_performance_tests()
    else:
        result = runner.run_all_tests(coverage=args.coverage)

    # Exit with appropriate code
    if result['failed'] > 0:
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    main()