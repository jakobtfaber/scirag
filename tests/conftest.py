"""
    --- AUTO-GENERATED DOCSTRING ---
    Table of content is automatically generated by Agent Docstrings v1.3.5
    
    Classes/Functions:
        - sample_text() (line 53)
        - sample_equations() (line 69)
        - sample_documents() (line 81)
        - temp_dir() (line 90)
        - enhanced_scirag() (line 97)
        - mathematical_processor() (line 113)
        - content_classifier() (line 120)
        - enhanced_chunker() (line 127)
        - asset_processor() (line 134)
        - glossary_extractor() (line 141)
        - pytest_configure(config) (line 149)
        - pytest_collection_modifyitems(config, items) (line 185)
        - TestUtils (line 207):
            - create_test_chunk(text: str, content_type: str = 'prose') -> Dict[str, Any] (line 211)
            - assert_chunk_valid(chunk) -> bool (line 226)
            - create_test_document(content: str, filename: str = 'test.txt') -> Path (line 237)
        - PerformanceTest (line 248):
            - measure_time(func, *args, **kwargs) (line 252)
            - measure_memory() (line 261)
        - ErrorTest (line 270):
            - test_invalid_inputs(processor, invalid_inputs: List[Any]) (line 274)
        - ConfigTest (line 293):
            - test_config_validation(config_class) (line 297)
            - test_config_defaults(config_instance) (line 307)
    --- END AUTO-GENERATED DOCSTRING ---

Enhanced SciRAG Test Configuration and Fixtures

This module provides shared test configuration, fixtures, and utilities
for all test modules in the Enhanced SciRAG test suite.
"""
import pytest
import tempfile
import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
import json

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Test data and fixtures


@pytest.fixture
def sample_text():
    """Sample text for testing."""
    return """
    This is a sample scientific document containing mathematical equations.

    The famous equation E = mc^2 represents the relationship between energy and mass.

    Figure 1: A diagram showing the process flow.

    Table 2: Experimental results are shown below.

    The term 'relativity' refers to Einstein's theory of special relativity.
    """


@pytest.fixture
def sample_equations():
    """Sample mathematical equations for testing."""
    return [
        "E = mc^2",
        "\\frac{d}{dx}[f(x)] = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}",
        "\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}",
        "\\sum_{i=1}^{n} i = \\frac{n(n+1)}{2}",
        "\\nabla \\times \\vec{E} = -\\frac{\\partial \\vec{B}}{\\partial t}"
    ]


@pytest.fixture
def sample_documents():
    """Sample document paths for testing."""
    txt_files_dir = project_root / "txt_files"
    if txt_files_dir.exists():
        return list(txt_files_dir.glob("*.txt"))[:3]  # First 3 documents
    return []


@pytest.fixture
def temp_dir():
    """Temporary directory for test files."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


@pytest.fixture
def enhanced_scirag():
    """Enhanced SciRAG instance for testing."""
    from scirag import SciRagEnhanced
    return SciRagEnhanced(
        enable_enhanced_processing=True,
        enable_mathematical_processing=True,
        enable_asset_processing=True,
        enable_glossary_extraction=True,
        enable_enhanced_chunking=True,
        chunk_size=1000,
        chunk_overlap=200,
        fallback_on_error=True
    )


@pytest.fixture
def mathematical_processor():
    """Mathematical processor for testing."""
    from scirag.enhanced_processing import MathematicalProcessor
    return MathematicalProcessor(enable_sympy=True)


@pytest.fixture
def content_classifier():
    """Content classifier for testing."""
    from scirag.enhanced_processing import ContentClassifier
    return ContentClassifier()


@pytest.fixture
def enhanced_chunker():
    """Enhanced chunker for testing."""
    from scirag.enhanced_processing import EnhancedChunker
    return EnhancedChunker(chunk_size=1000, overlap_ratio=0.2)


@pytest.fixture
def asset_processor():
    """Asset processor for testing."""
    from scirag.enhanced_processing import AssetProcessor
    return AssetProcessor()


@pytest.fixture
def glossary_extractor():
    """Glossary extractor for testing."""
    from scirag.enhanced_processing import GlossaryExtractor
    return GlossaryExtractor()

# Test markers


def pytest_configure(config):
    """Configure pytest with custom markers."""
    config.addinivalue_line(
        "markers", "unit: Unit tests for individual components"
    )
    config.addinivalue_line(
        "markers", "integration: Integration tests for component interactions"
    )
    config.addinivalue_line(
        "markers", "performance: Performance and benchmark tests"
    )
    config.addinivalue_line(
        "markers", "real_documents: Tests using real scientific documents"
    )
    config.addinivalue_line(
        "markers", "backward_compatibility: Tests for backward compatibility"
    )
    config.addinivalue_line(
        "markers", "error_handling: Tests for error handling and edge cases"
    )
    config.addinivalue_line(
        "markers", "configuration: Tests for configuration and settings"
    )
    config.addinivalue_line(
        "markers", "slow: Tests that take a long time to run"
    )
    config.addinivalue_line(
        "markers", "requires_gpu: Tests that require GPU resources"
    )
    config.addinivalue_line(
        "markers", "requires_credentials: Tests that require API credentials"
    )

# Test collection hooks


def pytest_collection_modifyitems(config, items):
    """Modify test collection to add markers based on test names."""
    for item in items:
        # Add markers based on test file names
        if "unit" in item.nodeid:
            item.add_marker(pytest.mark.unit)
        if "integration" in item.nodeid:
            item.add_marker(pytest.mark.integration)
        if "performance" in item.nodeid:
            item.add_marker(pytest.mark.performance)
        if "real_document" in item.nodeid:
            item.add_marker(pytest.mark.real_documents)
        if "backward_compatibility" in item.nodeid:
            item.add_marker(pytest.mark.backward_compatibility)
        if "error_handling" in item.nodeid:
            item.add_marker(pytest.mark.error_handling)
        if "configuration" in item.nodeid:
            item.add_marker(pytest.mark.configuration)

# Test utilities


class TestUtils:
    """Utility functions for tests."""

    @staticmethod
    def create_test_chunk(
            text: str, content_type: str = "prose") -> Dict[str, Any]:
        """Create a test chunk for testing purposes."""
        from scirag.enhanced_processing.enhanced_chunk import ContentType, EnhancedChunk

        return EnhancedChunk(
            id="test_chunk_1",
            text=text,
            source_id="test_doc",
            chunk_index=0,
            content_type=ContentType(content_type),
            confidence=0.8
        )

    @staticmethod
    def assert_chunk_valid(chunk) -> bool:
        """Assert that a chunk is valid."""
        assert chunk.id is not None
        assert chunk.text is not None
        assert chunk.source_id is not None
        assert chunk.chunk_index >= 0
        assert chunk.content_type is not None
        assert 0 <= chunk.confidence <= 1
        return True

    @staticmethod
    def create_test_document(content: str, filename: str = "test.txt") -> Path:
        """Create a temporary test document."""
        temp_file = tempfile.NamedTemporaryFile(
            mode='w', suffix='.txt', delete=False)
        temp_file.write(content)
        temp_file.close()
        return Path(temp_file.name)

# Performance testing utilities


class PerformanceTest:
    """Utilities for performance testing."""

    @staticmethod
    def measure_time(func, *args, **kwargs):
        """Measure execution time of a function."""
        import time
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        return result, end_time - start_time

    @staticmethod
    def measure_memory():
        """Measure current memory usage."""
        import psutil
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024  # MB

# Error testing utilities


class ErrorTest:
    """Utilities for error testing."""

    @staticmethod
    def test_invalid_inputs(processor, invalid_inputs: List[Any]):
        """Test processor with various invalid inputs."""
        for invalid_input in invalid_inputs:
            try:
                result = processor.process(invalid_input)
                # Should either return a valid result or raise a specific
                # exception
                assert result is not None
            except (ValueError, TypeError, AttributeError):
                # These are expected for invalid inputs
                pass
            except Exception as e:
                # Unexpected exception
                pytest.fail(
                    f"Unexpected exception for input {invalid_input}: {e}")

# Configuration testing utilities


class ConfigTest:
    """Utilities for configuration testing."""

    @staticmethod
    def test_config_validation(config_class):
        """Test configuration validation."""
        errors = config_class.validate_config()
        assert isinstance(errors, list)
        # Should not have critical errors
        critical_errors = [e for e in errors if "must be" in e.lower()]
        assert len(
            critical_errors) == 0, f"Critical config errors: {critical_errors}"

    @staticmethod
    def test_config_defaults(config_instance):
        """Test configuration default values."""
        assert hasattr(config_instance, 'ENABLE_ENHANCED_PROCESSING')
        assert hasattr(config_instance, 'ENABLE_MATHEMATICAL_PROCESSING')
        assert hasattr(config_instance, 'ENABLE_ASSET_PROCESSING')
        assert hasattr(config_instance, 'ENABLE_GLOSSARY_EXTRACTION')